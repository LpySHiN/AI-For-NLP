{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Corpus Based Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus_path = \"C:\\\\Users\\\\LpySHiN\\\\AI-NLP\\\\wiki_divide\\\\AA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(wiki_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1254', 'confidence': 0.24576554947717053, 'language': 'Turkish'}\n"
     ]
    }
   ],
   "source": [
    "# 编码检测\n",
    "import chardet, csv\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "# with open(database, 'rb') as k:\n",
    "#     print(chardet.detect(k.read(10000)))\n",
    "\n",
    "usock = open(wiki_corpus_path, 'rb')\n",
    "detector = UniversalDetector()\n",
    "for line in usock.readlines():\n",
    "    detector.feed(line)\n",
    "    if detector.done:\n",
    "        break\n",
    "detector.close()\n",
    "usock.close()\n",
    "print(detector.result)\n",
    "# chardet_rst = pd.read_csv(database, encoding='GB2312')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "wiki_corpus_path_pattern = wiki_corpus_path + '\\\\wiki_**'\n",
    "files = glob.glob(wiki_corpus_path_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取wiki->list\n",
    "all_articles = list()\n",
    "for name in files:\n",
    "    try:\n",
    "        f = open(name, 'r', encoding='utf8')\n",
    "        lines = f.readlines()\n",
    "        all_articles.append(lines)\n",
    "    except Exception as E:\n",
    "        print(E.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_tokens(string):\n",
    "    return ' '.join(re.findall('[\\w|\\d]+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696332\n",
      "836072\n",
      "984568\n",
      "1037490\n",
      "1158947\n",
      "1173605\n",
      "1175800\n",
      "1908933\n",
      "1501687\n",
      "1192449\n",
      "1237096\n",
      "1224260\n",
      "763134\n",
      "0 14890373\n"
     ]
    }
   ],
   "source": [
    "sum_len = 0\n",
    "for articles in all_articles:\n",
    "    sum_len += len(articles)\n",
    "    print(len(articles))\n",
    "sum_len\n",
    "print(0,sum_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 696332\n",
      "File[1] Finish 0.0%\n",
      "File[1] Finish 14.360965746224503%\n",
      "File[1] Finish 28.721931492449006%\n",
      "File[1] Finish 43.0828972386735%\n",
      "File[1] Finish 57.44386298489801%\n",
      "File[1] Finish 71.80482873112251%\n",
      "File[1] Finish 86.165794477347%\n",
      "Finish [0]\n",
      "1 836072\n",
      "File[2] Finish 0.0%\n",
      "File[2] Finish 11.960692380560525%\n",
      "File[2] Finish 23.92138476112105%\n",
      "File[2] Finish 35.88207714168158%\n",
      "File[2] Finish 47.8427695222421%\n",
      "File[2] Finish 59.80346190280263%\n",
      "File[2] Finish 71.76415428336315%\n",
      "File[2] Finish 83.72484666392369%\n",
      "File[2] Finish 95.6855390444842%\n",
      "Finish [1]\n",
      "2 984568\n",
      "File[3] Finish 0.0%\n",
      "File[3] Finish 10.156738793054416%\n",
      "File[3] Finish 20.31347758610883%\n",
      "File[3] Finish 30.470216379163247%\n",
      "File[3] Finish 40.62695517221766%\n",
      "File[3] Finish 50.78369396527208%\n",
      "File[3] Finish 60.940432758326494%\n",
      "File[3] Finish 71.09717155138091%\n",
      "File[3] Finish 81.25391034443533%\n",
      "File[3] Finish 91.41064913748974%\n",
      "Finish [2]\n"
     ]
    }
   ],
   "source": [
    "# 预处理\n",
    "TEXT = ''\n",
    "i = 0\n",
    "for articles in all_articles[:3]:\n",
    "    print(i, len(articles))\n",
    "    file_count = 0\n",
    "    for element in articles:\n",
    "        if (file_count % 100000) == 0:\n",
    "            print('File[{}] Finish {}%'.format(i + 1, file_count * 100.00 / len(articles)))\n",
    "        file_count += 1\n",
    "        if re.match('<doc id|</doc>', element) is not None:\n",
    "            continue\n",
    "        else:\n",
    "            TEXT += element\n",
    "    if file_count == len(articles):\n",
    "        print('Finish [{}]'.format(i))\n",
    "    i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_TOKENS = find_all_tokens(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繁体->简体\n",
    "from hanziconv import HanziConv\n",
    "TEXT_TOKENS = HanziConv.toSimplified(TEXT_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import jieba\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "ALL_TOKENS = [i for i in cut(TEXT_TOKENS) if i.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the frequeces of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(ALL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 2809970),\n",
       " ('在', 764020),\n",
       " ('年', 674711),\n",
       " ('是', 540321),\n",
       " ('和', 420283),\n",
       " ('了', 363728),\n",
       " ('为', 350624),\n",
       " ('月', 282922),\n",
       " ('与', 270505),\n",
       " ('有', 248615)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences_all = [f for w, f in words_count.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequences_sum = sum(frequences_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESP = 1 / frequences_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(word):\n",
    "    if word in words_count:\n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return ESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_1_gram(string):\n",
    "    words = cut(string)\n",
    "    return product(get_prob(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3929063423019209e-18"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_1_gram('我真是太累了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 4.086292655830849e-54\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 6.032069023718534e-52\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 5.328096244532081e-26\n",
      "---- 真是一只好看的小猫 with probility 3.720357960077989e-23\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 我去吃火锅，今晚 with probility 1.2894577206106787e-28\n",
      "---- 今晚我去吃火锅 with probility 6.583922766773603e-21\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"我去吃火锅，今晚 今晚我去吃火锅\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = lm_1_gram(s1), lm_1_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_grams_words = [''.join(ALL_TOKENS[i:i + 2]) for i in range(len(ALL_TOKENS[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter: return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001248439499587882"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prob_2_gram('写', '书包')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_2_gram(sentence):\n",
    "    sentence_prob = 1\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i - 1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_prob *= prob\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天晚上请你吃大餐，我们一起吃日料 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 5.182488934711886e-29\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 7.403555621016978e-30\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 4.4584441739465745e-21\n",
      "---- 真是一只好看的小猫 with probility 1.5825246975550127e-17\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 1.7768601708927513e-14\n",
      "---- 今晚火锅去吃我 with probility 2.865662810946662e-16\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 3.1714803518278e-13\n",
      "---- 养乐多绿来一杯 with probility 1.9584947565513632e-08\n",
      "今天天气居然是晴天 is more possible\n",
      "---- 今天天气居然是晴天 with probility 2.0854367062102974e-14\n",
      "---- 大使馆天气居然是乌鸦 with probility 8.082461777756108e-18\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\",\n",
    "    \"今天天气居然是晴天 大使馆天气居然是乌鸦\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = lm_2_gram(s1), lm_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-gram\n",
    "内存不够跑了，老师帮忙看下有没有问题吧~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-1f44bfb42c70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_3_grams_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mALL_TOKENS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mALL_TOKENS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_3_grams_words = [''.join(ALL_TOKENS[i:i + 3]) for i in range(len(ALL_TOKENS[:-3]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_3_gram_sum = len(all_3_grams_words)\n",
    "_3_gram_counter = Counter(all_3_grams_words)\n",
    "\n",
    "def get_3_combination_prob(w1, w2, w3):\n",
    "    if w1 + w2 + w3 in _3_gram_counter: return _3_gram_counter[w1 + w2 + w3] / _3_gram_sum\n",
    "    else:\n",
    "        return 1 / _3_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_3_gram(w1, w2, w3):\n",
    "    return get_3_combination_prob(w1, w2, w3) / get_combination_prob(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_3_gram(sentence):\n",
    "    sentence_prob = 1\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        elif i == 1:\n",
    "            prob = get_prob_2_gram(words[i - 1], word)\n",
    "        else:\n",
    "            prob = get_prob_2_gram(words[i - 2], words[i - 3], word)\n",
    "        sentence_prob *= prob\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we need to solve following problems, how can language model help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Voice Recognization\n",
    "\n",
    "语言模型可将语音识别转化的文本进行纠错，通过分析低概率词组进行更正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sogou pinyin input\n",
    "\n",
    "优化语言模型，新增pinyin-word字典，通过语言模型计算pinyin生成句的概率，按概率排序推荐为输入选项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Auto correction in search engine\n",
    "\n",
    "判断用户搜索输入的句子，通过语言模型对句子中概率较低的字/两连词组/三连词组进行替换，再进行搜索并推荐搜索纠错后的句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Abnormal Detection\n",
    "\n",
    "例如在语音对话机器人模型中，针对语音输入部分进行语言模型分析，将概率过低的输入作为异常分支，采用特殊回复处理此分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "优点： 基于概率的方法编码较为容易，可以解决大部分问题，而且准确率很高；\n",
    "\n",
    "缺点： 基于概率的方法其本质是基于语料库，语料库的概率特性将会影响概率模型的分析结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve *OOV* problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: How did you solve this problem in your programming task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "+ 通过返回固定概率(1/frequece_sum)表征OOV词组/字\n",
    "\n",
    "Others method:\n",
    "    \n",
    "+ 增大语料库丰富性\n",
    "+ 在语言模型输入时自动检测OOV词组，将该词组用同类别词语替代后再进行语言模型分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Read about the 'Good-Turing Estimator', can explain the main points about this method, and may implement this method in your programming task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "老师如果能看到这里的话，我英语比较差，没看到Good-Turing的那个method，没明白每个公式的参数的含义，如果可以的话，能不能出个翻译版或者课上单独讲一下呀？如下，每个参数的含义。\n",
    "\n",
    "$$ N_{r}\\;=\\;\\vert \\{ x\\;\\vert\\;R_{x}=\\;r\\} \\vert $$\n",
    "\n",
    "$$ N\\;=\\;\\sum_{r=1}^{\\infty}{r \\cdot Nr}$$\n",
    "\n",
    "$$ p_0\\;=\\;\\frac{N1}{N}$$\n",
    "\n",
    "$$ p_r\\;=\\;\\frac{(r+1) \\cdot S(N_{r+1})}{N \\cdot S(N_r)}$$\n",
    "\n",
    "$$ z_r\\;=\\;\\frac{N_r}{0.5 \\cdot (t\\,-\\,q)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
